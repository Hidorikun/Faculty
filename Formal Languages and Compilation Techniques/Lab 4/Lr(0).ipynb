{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <u>Grammar</u> is defined as G = (N, E, P, S) where:\n",
    "    \n",
    "    N = set of non-terminals\n",
    "    E = set of terminals\n",
    "    P = set of productions\n",
    "    S = starting symbol \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar:\n",
    "    @staticmethod\n",
    "    def parseLine(line):\n",
    "        equalPos = line.index('=')\n",
    "        rhs = line[equalPos + 1:].strip('\\n').strip(' ')[1:-1]\n",
    "        return [symbol.strip() for symbol in rhs.split(',')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def fromFile(fileName):\n",
    "        with open(fileName) as file: \n",
    "            N = Grammar.parseLine(file.readline())\n",
    "            E = Grammar.parseLine(file.readline())\n",
    "            S = file.readline().split('=')[1].strip()\n",
    "            P = Grammar.parseRules([line.strip('\\n').strip(' ').strip(',') for line in file][1:-1])\n",
    "            \n",
    "            return Grammar(N, E, P, S)\n",
    "\n",
    "    @staticmethod        \n",
    "    def parseRules(rules):\n",
    "        result = []\n",
    "        \n",
    "        for rule in rules:\n",
    "            lhs, rhs = rule.split('->')\n",
    "            lhs = lhs.strip()\n",
    "            rhs = [ value.strip() for value in rhs.split('|')]\n",
    "            \n",
    "            for value in rhs: \n",
    "                result.append((lhs, value.split()))\n",
    "        \n",
    "        return result \n",
    "   \n",
    "    def __init__(self, N, E, P, S):\n",
    "        self.N = N \n",
    "        self.E = E\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        \n",
    "    def isNonTerminal(self, value):\n",
    "        return value in self.N\n",
    "    \n",
    "    def isTerminal(self, value):\n",
    "        return value in self.E \n",
    "    \n",
    "    def isRegular(self):\n",
    "        usedInRhs = dict() \n",
    "        notAllowedInRhs = list() \n",
    "        \n",
    "        for rule in self.P: \n",
    "            lhs, rhs = rule\n",
    "            hasTerminal = False \n",
    "            hasNonTerminal = False\n",
    "            for char in rhs: \n",
    "                if self.isNonTerminal(char): \n",
    "                    usedInRhs[char] = True\n",
    "                    hasNonTerminal = True\n",
    "                elif self.isTerminal(char): \n",
    "                    if hasNonTerminal: \n",
    "                        return False\n",
    "                    hasTerminal = True \n",
    "                if char == 'E': \n",
    "                    notAllowedInRhs.append(lhs)\n",
    "                    \n",
    "            if hasNonTerminal and not hasTerminal: \n",
    "                return False\n",
    "        \n",
    "        for char in notAllowedInRhs: \n",
    "            if char in usedInRhs: \n",
    "                return False \n",
    "            \n",
    "        return True\n",
    "   \n",
    "    def getProductionsFor(self, nonTerminal): \n",
    "        if not self.isNonTerminal(nonTerminal):\n",
    "            raise Exception('Can only show productions for non-terminals')\n",
    "            \n",
    "        return [ prod for prod in self.P if prod[0] == nonTerminal ]\n",
    "    \n",
    "    def showProductionsFor(self, nonTerminal):\n",
    "        productions = self.getProductionsFor(nonTerminal)\n",
    "        \n",
    "        print(', '.join([' -> '.join(prod) for prod in productions]))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'N = { ' + ', '.join(self.N) + ' }\\n' \\\n",
    "             + 'E = { ' + ', '.join(self.E) + ' }\\n' \\\n",
    "             + 'P = { ' + ', '.join([' -> '.join([prod[0], ' '.join(prod[1])]) for prod in self.P]) + ' }\\n' \\\n",
    "             + 'S = ' + str(self.S) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR(0) Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For parsing the method from the previous labs was used. Tokenizing the input and then obtaining the resulting PIF.\n",
    "PIF conversion is done based on a codification tables with each non-terminal being map to a unique id, together with the\n",
    "codes found in the symbol and constant tables. The grammar implementation and code is also the one used in the previous\n",
    "work, also including regular grammar verification.\n",
    "The PIF is then used in the LR(0) parsing algorithm as detailed in the course slides as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closure Computation \n",
    "\n",
    "The closure of a set of LR(0) items can be computed using a simple (but important) little algorithm\n",
    "Algorithm to compute closure(A)\n",
    "\tset A' equal to A.\n",
    "\twhile there is some [ N -> beta1 . M beta2 ] exists in A' such that\n",
    "\tM->beta3 exists in P and [ M ->.beta3 ] exists in A' add [M ->.beta3 ] to A'. \n",
    "\n",
    "\t\n",
    "The definition of the LR(0) machine is:\n",
    "\n",
    "Let the set of states of the machine be the set of all sets of LR(0) items for G.\n",
    "Let the set of final states be all states except the state corresponding to the empty set of LR(0) item.\n",
    "Let the initial state be the state corresponding to the set\n",
    "\n",
    "\tclosure ( {[ S' ->.S$]} ) \n",
    "\n",
    "Let the transition function f: pi(x)(Vn U Vt) -> pi be defined by:\n",
    "\n",
    "\tf(pi,x) = closure(goto(pi,x)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reguli tabel LR(0)\n",
    "\n",
    "    1.dacă [A → α.β] ∈si atunci acțiune(si)=shift\n",
    "\t2.dacă [A → β.] ∈si și A ≠Sʹ atunci acțiune(si)=reduce l, unde l -numărul producției A → β\n",
    "\t3.dacă [Sʹ → S.] ∈si atunci acțiune(si)=acc\n",
    "\t4.dacă goto(si, X) = sj atunci goto(si, X) = sj\n",
    "\t5.toate celelalte valori = eroare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lr0Parser: \n",
    "    \n",
    "    def __init__(self, grammar): \n",
    "        self.__grammar = grammar\n",
    "        self.__workingStack = []\n",
    "        self.__inputStack = []\n",
    "        self.__output = [] \n",
    "        \n",
    "    def closure(self, productions): \n",
    "        \n",
    "        if productions == []:\n",
    "            return None\n",
    "        C = productions\n",
    "        finished = False \n",
    "        \n",
    "        while not finished:\n",
    "            finished = True \n",
    "            for dottedProd in C:\n",
    "                dotIndex = dottedProd[1].index('.')\n",
    "                alpha = dottedProd[1][:dotIndex]\n",
    "                Bbeta = dottedProd[1][dotIndex + 1:]\n",
    "\n",
    "                if len(Bbeta) == 0: \n",
    "                    continue\n",
    "                    \n",
    "                B = Bbeta[0]\n",
    "                if self.__grammar.isTerminal(B): \n",
    "                    continue\n",
    "                    \n",
    "                for prod in self.__grammar.getProductionsFor(B):\n",
    "                    dottedProd = (prod[0], ['.'] + prod[1])\n",
    "                    if dottedProd not in C: \n",
    "                        C += [ dottedProd ]\n",
    "                        finished = False \n",
    "        return C\n",
    "        \n",
    "    \n",
    "    def goTo(self, state, symbol):\n",
    "        C = []\n",
    "        \n",
    "        for dottedProd in state: \n",
    "            dotIndex = dottedProd[1].index('.')\n",
    "            alpha = dottedProd[1][:dotIndex]\n",
    "            Xbeta = dottedProd[1][dotIndex + 1:]\n",
    "            \n",
    "            if len(Xbeta) == 0: \n",
    "                continue\n",
    "            \n",
    "            X, beta = Xbeta[0], Xbeta[1:]\n",
    "            \n",
    "            if X == symbol:\n",
    "                resultProd = (dottedProd[0], alpha + [X] + ['.'] + beta)\n",
    "                C = C + [ resultProd ]\n",
    "        \n",
    "        return self.closure(C)\n",
    "        \n",
    "    def getCannonicalCollection(self): \n",
    "        C = [ self.closure([('S1', ['.', self.__grammar.S])]) ]\n",
    "        \n",
    "        finished = False \n",
    "        \n",
    "        while not finished: \n",
    "            finished = True \n",
    "            \n",
    "            for state in C: \n",
    "                for symbol in self.__grammar.N + self.__grammar.E: \n",
    "                    nextState = self.goTo(state, symbol)\n",
    "                    if nextState is not None and nextState not in C: \n",
    "                        C = C + [ nextState ]\n",
    "                        finished = False\n",
    "    \n",
    "        return C\n",
    "    \n",
    "    def getTable(self): \n",
    "        states = self.getCannonicalCollection()\n",
    "        table = [{} for _ in range(len(states))]\n",
    "        \n",
    "        for index, state in enumerate(states):\n",
    "            meetsFirstRule = 0\n",
    "            meetsSecondRule = 0 \n",
    "            meetsThirdRule = 0 \n",
    "            \n",
    "            for prod in state: \n",
    "                dotIndex = prod[1].index('.')\n",
    "                alpha = prod[1][:dotIndex]\n",
    "                beta = prod[1][dotIndex + 1:]\n",
    "                if len(beta) != 0:\n",
    "                    meetsFirstRule += 1\n",
    "                else:\n",
    "                    if prod[0] != 'S1':\n",
    "                        meetsSecondRule += 1\n",
    "                        productionIndex = self.__grammar.P.index((prod[0], alpha))\n",
    "                    elif alpha == [self.__grammar.S]: \n",
    "                        meetsThirdRule += 1\n",
    "                \n",
    "            if meetsFirstRule == len(state): \n",
    "                table[index]['action'] = 'shift'\n",
    "                \n",
    "            elif meetsSecondRule == len(state):\n",
    "                table[index]['action'] = 'reduce ' + str(productionIndex)\n",
    "                \n",
    "            elif meetsThirdRule == len(state):\n",
    "                table[index]['action'] = 'acc'\n",
    "                \n",
    "            else: \n",
    "                raise(Exception('No action detected for state ' + str(index) + ' ' + str(state)))\n",
    "                \n",
    "            \n",
    "            for symbol in self.__grammar.N + self.__grammar.E: \n",
    "                nextState = self.goTo(state, symbol)\n",
    "                if nextState in states: \n",
    "                    table[index][symbol] = states.index(nextState)\n",
    "            \n",
    "        return table\n",
    "    \n",
    "    \n",
    "    def parse(self, inputSequence): \n",
    "        table = self.getTable()\n",
    "        \n",
    "        self.__workingStack = ['0']\n",
    "        self.__inputStack = [symbol for symbol in inputSequence]\n",
    "        self.__output = []\n",
    "        \n",
    "        while len(self.__workingStack) != 0: \n",
    "            state = int(self.__workingStack[-1])\n",
    "            if len(self.__inputStack) > 0:\n",
    "                symbol = self.__inputStack.pop(0)\n",
    "            else: \n",
    "                symbol = None\n",
    "                \n",
    "            if table[state]['action'] == 'shift': \n",
    "                if symbol not in table[state]: \n",
    "                    print('workstack', self.__workingStack)\n",
    "                    print('inputstack', self.__inputStack)\n",
    "                    print('state', state)\n",
    "                    print('symbol', symbol)\n",
    "                    \n",
    "                    raise(Exception('Cannot parse shift'))\n",
    "                self.__workingStack.append(symbol)\n",
    "                self.__workingStack.append(table[state][symbol])\n",
    "                \n",
    "            elif table[state]['action'] == 'acc': \n",
    "                if len(self.__inputStack) != 0:\n",
    "                    raise(Exception('Cannot Parse acc'))\n",
    "                \n",
    "                self.__workingStack.clear()\n",
    "                \n",
    "            else: \n",
    "                reducedState = int(table[state]['action'].split(' ')[1])\n",
    "                reducedProduction = self.__grammar.P[reducedState]\n",
    "                \n",
    "                toRemoveFromWorkingStack = [symbol for symbol in reducedProduction[1]]\n",
    "                \n",
    "                while len(toRemoveFromWorkingStack) > 0 and len(self.__workingStack) > 0:\n",
    "                    if self.__workingStack[-1] == toRemoveFromWorkingStack[-1]: \n",
    "                        toRemoveFromWorkingStack.pop()\n",
    "                    self.__workingStack.pop()\n",
    "                    \n",
    "                if len(toRemoveFromWorkingStack) != 0: \n",
    "                    raise(Exception('Cannot Parse reduce'))\n",
    "                \n",
    "                self.__inputStack.insert(0, reducedProduction[0])\n",
    "                self.__output.insert(0, str(reducedState))\n",
    "            \n",
    "        return self.__output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicl Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPartOfOperator(char): \n",
    "    for op in operators: \n",
    "        if char in op: \n",
    "            return True \n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEscapedQuote(line, index):\n",
    "    return False if index == 0 else line[index -1] == '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStringToken(line, index):\n",
    "    token = ''\n",
    "    quoteCount = 0\n",
    "    \n",
    "    while index < len(line) and quoteCount < 2:\n",
    "        if line[index] == '\"' and not isEscapedQuote(line, index):\n",
    "            quoteCount += 1 \n",
    "        token += line[index]\n",
    "        index += 1\n",
    "        \n",
    "    return token, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOperatorToken(line, index):\n",
    "    token = ''\n",
    "    \n",
    "    while index < len(line) and isPartOfOperator(line[index]):\n",
    "        token += line[index]\n",
    "        index += 1 \n",
    "        \n",
    "    return token, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenGenerator(line, separators): \n",
    "    token = ''\n",
    "    index = 0\n",
    "    \n",
    "    while index < len(line):\n",
    "        if line[index] == '\"': \n",
    "            if token: \n",
    "                yield token\n",
    "            token, index = getStringToken(line, index)\n",
    "            yield token\n",
    "            token = ''\n",
    "            \n",
    "        elif isPartOfOperator(line[index]):\n",
    "            if token: \n",
    "                yield token\n",
    "            token, index = getOperatorToken(line, index)\n",
    "            yield token\n",
    "            token = ''\n",
    "        \n",
    "        elif line[index] in separators:\n",
    "            if token: \n",
    "                yield token\n",
    "            token, index = line[index], index + 1\n",
    "            yield token\n",
    "            token = ''\n",
    "            \n",
    "        else:\n",
    "            token += line[index]\n",
    "            index += 1 \n",
    "    if token: \n",
    "        yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our language specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = ['[', ']', '{', '}', '(', ')', ';', ' '] \n",
    "operators = ['+', '-', '*', '/', '<', '<=', '=', '>=', '>', '>>', '<<', '==', '&&', '||', '!', '&']\n",
    "reservedWords = ['int', 'char', 'bool', 'float', 'array', 'struct', 'if', 'else', 'for', 'while', 'cout', 'true', 'false']\n",
    "\n",
    "everything = separators + operators + reservedWords\n",
    "codification = dict( [ [everything[i], i + 2] for i in range(len(everything))])\n",
    "codification['identifier'] = 0\n",
    "codification['constant'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating PIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolTable():\n",
    "    def __init__(self):\n",
    "        self.__content = dict() \n",
    "        self.__count = -1\n",
    "        \n",
    "    def add(self, value):\n",
    "        self.__count += 1\n",
    "        self.__content[self.__count] = value\n",
    "        return self.__count\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgramInternalForm: \n",
    "    def __init__(self):\n",
    "        self.__content = []\n",
    "        \n",
    "    def add(self, code, id):\n",
    "        self.__content.append((code, id))\n",
    "        \n",
    "    def getCodes(self): \n",
    "        return [ code[0] for code in self.__content]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIdentifier(token): \n",
    "    return re.match(r'^[a-zA-Z]([a-zA-Z]|[0-9]|_){,8}$', token) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isConstant(token):\n",
    "    return re.match('^(0|[\\+\\-]?[1-9][0-9]*)$|^\\'.\\'$|^\\\".*\\\"$', token) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifierTable = SymbolTable()\n",
    "constantsTable = SymbolTable()\n",
    "pif = ProgramInternalForm()\n",
    "\n",
    "fileName = 'program.txt'\n",
    "\n",
    "with open(fileName, 'r') as file: \n",
    "    lineNo = 0\n",
    "    for line in file:\n",
    "        lineNo += 1 \n",
    "        for token in tokenGenerator(line.strip('\\n'), separators): \n",
    "            if token == ' ': \n",
    "                continue\n",
    "            if token in separators + operators + reservedWords: \n",
    "                pif.add(codification[token], -1)\n",
    "            elif isIdentifier(token): \n",
    "                id = identifierTable.add(token)\n",
    "                pif.add(codification['identifier'], id)\n",
    "            elif isConstant(token): \n",
    "                id = constantsTable.add(token)\n",
    "                pif.add(codification['constant'], id)\n",
    "            else:\n",
    "                raise Exception('Unknown token ' + token + ' at line ' + str(lineNo)) \n",
    "                \n",
    "print('Program Internal Form: \\n', pif)\n",
    "print('Identifier Table: \\n', identifierTable) \n",
    "print('Constants Table: \\n', constantsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning PIF into an input stack for Lr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverseCodification = dict( [ [codification[key], key] for key in codification])\n",
    "\n",
    "for code in pif.getCodes(): \n",
    "    print(code, ' : ', inverseCodification[code])\n",
    "    \n",
    "inputStack = [str(code) for code in pif.getCodes()]\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying symbol codification from language specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in codification:\n",
    "    print(key, ' : ', codification[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Lr(0) Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, line in enumerate(lr0.getTable()): \n",
    "    print(index, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = lr0.closure([ ('S1', ['.', g.S]) ])\n",
    "\n",
    "for s in g.N + g.E: \n",
    "    print('goto( s0, ' + s + ') = ', lr0.goTo(s0, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Cannonical Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Grammar.fromFile(\"grammar.txt\")\n",
    "lr0 = Lr0Parser(g)\n",
    "\n",
    "for index, state in enumerate(lr0.getCannonicalCollection()):\n",
    "    print(index, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lr(0) Parser on the input stack generated by PIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Grammar.fromFile(\"my_grammar.txt\")\n",
    "g.P = [('S1', ['.', g.S])] + g.P\n",
    "g.N += ['S1']\n",
    "lr0 = Lr0Parser(g)\n",
    "\n",
    "print(g)\n",
    "\n",
    "print(lr0.parse(inputStack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
